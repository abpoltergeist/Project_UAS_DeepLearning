{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES = 4  \n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class GenreResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=NUM_CLASSES):\n",
    "        super(GenreResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def resnet18_genre_classifier():\n",
    "    \"\"\"Constructs a ResNet-18 model for genre classification.\"\"\"\n",
    "    return GenreResNet(BasicBlock, [2, 2, 2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf793c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def preprocess_audio(audio_path, duration=30):\n",
    "    \"\"\"\n",
    "    1. Loads an audio file.\n",
    "    2. Trims it to the specified duration (in seconds).\n",
    "    3. Converts it into a Mel spectrogram.\n",
    "    4. Applies the necessary transformations for the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        y, sr = librosa.load(audio_path, sr=None, mono=True, duration=duration)\n",
    "\n",
    "\n",
    "        if len(y) < sr * duration:\n",
    "            y = np.pad(y, (0, sr * duration - len(y)), 'constant')\n",
    "\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "        S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "\n",
    "        fig = plt.figure(figsize=(2.56, 2.56), dpi=100) \n",
    "        librosa.display.specshow(S_DB, sr=sr, cmap='magma')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout(pad=0)\n",
    "        \n",
    "\n",
    "        fig.canvas.draw()\n",
    "        img_data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        img = img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        plt.close(fig)\n",
    "        \n",
    "        pil_img = Image.fromarray(img)\n",
    "\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            \n",
    "            transforms.ToTensor()\n",
    "\n",
    "        ])\n",
    "        \n",
    "        image_tensor = transform(pil_img)\n",
    "        \n",
    "\n",
    "        return image_tensor.unsqueeze(0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio file: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_genre(model, audio_tensor, class_names):\n",
    "    \"\"\"\n",
    "    Takes a preprocessed audio tensor and returns the predicted genre.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(audio_tensor)\n",
    "        _, predicted_idx = torch.max(output, 1)\n",
    "        return class_names[predicted_idx.item()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    MODEL_PATH = '/Users/abynaya/Documents/best_genre_resnet18.pth' \n",
    "    AUDIO_FILE = '/Users/abynaya/Downloads/Green Day - Last Night on Earth.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    GENRE_CLASSES = ['classical', 'hiphop', 'pop', 'rock'] \n",
    "\n",
    "    # --- 1. Load Model ---\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Instantiate the model\n",
    "    model = resnet18_genre_classifier()\n",
    "    \n",
    "    # Load the saved state dictionary\n",
    "    # Use map_location to load the model on CPU if it was trained on GPU and you are now on CPU\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Model file not found at '{MODEL_PATH}'\")\n",
    "    elif not os.path.exists(AUDIO_FILE):\n",
    "        print(f\"Error: Audio file not found at '{AUDIO_FILE}'\")\n",
    "    else:\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "            model.to(device)\n",
    "            print(\"Model loaded successfully.\")\n",
    "\n",
    "            # --- 2. Preprocess Audio ---\n",
    "            print(\"Processing audio file...\")\n",
    "            audio_tensor = preprocess_audio(AUDIO_FILE)\n",
    "\n",
    "            if audio_tensor is not None:\n",
    "                audio_tensor = audio_tensor.to(device)\n",
    "\n",
    "                # --- 3. Predict Genre ---\n",
    "                predicted_genre = predict_genre(model, audio_tensor, GENRE_CLASSES)\n",
    "                print(\"\\n\" + \"=\"*30)\n",
    "                print(f\"The predicted genre for '{os.path.basename(AUDIO_FILE)}' is: {predicted_genre.upper()}\")\n",
    "                print(\"=\"*30)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during model loading or prediction: {e}\")\n",
    "            print(\"Please ensure the model architecture in this script matches the one used for training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
