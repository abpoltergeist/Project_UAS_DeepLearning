{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H0E-t1zqW3y",
        "outputId": "13e2ff52-7dda-4118-e1a8-25db0e2f239d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 4\n",
        "IMAGE_SIZE = 128\n",
        "DATA_PATH = '/Users/abynaya/code/project_akhir_uas/dataset dl/bener_output'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***OLD MODEL***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN Model (updated for 3-channel input)\n",
        "class GenreCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GenreCNN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),  # now using 3 input channels\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128 * (IMAGE_SIZE // 8) * (IMAGE_SIZE // 8), 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, NUM_CLASSES)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = resnet18_genre_classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), 'genre_cnn_model_rgb.pth')\n",
        "print(\"Model saved as 'genre_cnn_model_rgb.pth'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model after training\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/genre_cnn_baru.pth')\n",
        "print(\"Model saved to Google Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total dataset size: 430\n",
            "Training dataset size: 258\n",
            "Testing dataset size: 172\n",
            "Number of training batches: 9\n",
            "Number of testing batches: 6\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Define your data directory\n",
        "DATA_DIR = '/Users/abynaya/code/project_akhir_uas/dataset dl/bener_output'\n",
        "BATCH_SIZE = 32  # You can adjust your batch size here\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "])\n",
        "\n",
        "# 2. Load the full dataset using ImageFolder\n",
        "full_dataset = ImageFolder(DATA_DIR, transform=transform)\n",
        "\n",
        "# 3. Calculate split lengths\n",
        "dataset_size = len(full_dataset)\n",
        "train_size = int(0.6 * dataset_size)\n",
        "test_size = dataset_size - train_size # The remaining for the test set\n",
        "\n",
        "# 4. Perform the random split\n",
        "# random_split returns two Subset objects, which are like datasets but hold a subset of indices\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "# 5. Create DataLoaders for both train and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) # Shuffle training data\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) # No need to shuffle test data\n",
        "\n",
        "print(f\"Total dataset size: {dataset_size}\")\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of testing batches: {len(test_loader)}\")\n",
        "\n",
        "# You can iterate through your loaders like this:\n",
        "# for images, labels in train_loader:\n",
        "#     # Your training logic here\n",
        "#     pass\n",
        "\n",
        "# for images, labels in test_loader:\n",
        "#     # Your testing/evaluation logic here\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***NEW MODEL***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "\n",
        "# --- Global variables (replace with your actual values) ---\n",
        "IMAGE_SIZE = 128  # Example image size\n",
        "NUM_CLASSES = 4  # Example number of genre classes\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "class AlexNetGenreClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    An AlexNet-based model for genre classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super(AlexNetGenreClassifier, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def alexnet_genre_classifier():\n",
        "    \"\"\"\n",
        "    Constructs an AlexNet model for genre classification.\n",
        "    \"\"\"\n",
        "    return AlexNetGenreClassifier()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/160 | Train Loss: 1.4308 | Val Loss: 1.3899 | Validation Accuracy: 19.77%\n",
            "Validation loss improved. Saving best model.\n",
            "Epoch 2/160 | Train Loss: 1.3941 | Val Loss: 1.3715 | Validation Accuracy: 23.84%\n",
            "Validation loss improved. Saving best model.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     25\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/code/envir/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
            "File \u001b[0;32m~/code/envir/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/code/envir/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
            "File \u001b[0;32m~/code/envir/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
            "File \u001b[0;32m~/code/envir/lib/python3.12/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
            "File \u001b[0;32m~/code/envir/lib/python3.12/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/code/envir/lib/python3.12/site-packages/torchvision/datasets/folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/code/envir/lib/python3.12/site-packages/PIL/Image.py:941\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    891\u001b[0m     mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     colors: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[1;32m    897\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 941\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    945\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
            "File \u001b[0;32m~/code/envir/lib/python3.12/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# Assume resnet18_genre_classifier, train_loader, test_loader, device are defined\n",
        "\n",
        "EPOCHS = 160\n",
        "PATIENCE = 50  # Number of epochs to wait for improvement\n",
        "min_val_loss = float('inf') # Initialize with a very large number\n",
        "epochs_no_improve = 0 # Counter for epochs without improvement\n",
        "early_stop = False\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = alexnet_genre_classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    if early_stop:\n",
        "        print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "        break\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader) # Renamed for clarity\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    val_loss = 0 # Initialize validation loss for the current epoch\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels) # Calculate loss on validation set\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader) # Average validation loss\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Early Stopping Logic\n",
        "    if avg_val_loss < min_val_loss:\n",
        "        min_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # Optionally, save the best model weights here\n",
        "        torch.save(model.state_dict(), 'best_genre_alexnet.pth')\n",
        "        print(\"Validation loss improved. Saving best model.\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            early_stop = True\n",
        "\n",
        "# Load the best model if early stopping occurred and you saved it\n",
        "if early_stop:\n",
        "    print(\"Loading best model weights before final save.\")\n",
        "    model.load_state_dict(torch.load('best_genre_resnet18_model.pth'))\n",
        "    # You might want to rename the final saved model to reflect it's the best one\n",
        "    torch.save(model.state_dict(), 'final_early_stopped_genre_alexnet18_model.pth')\n",
        "    print(\"Final model saved as 'final_early_stopped_genre_alexnett18_model.pth'\")\n",
        "else:\n",
        "    # If training completed all epochs without early stopping\n",
        "    torch.save(model.state_dict(), 'genre_resnet18_model_full_epochs.pth')\n",
        "    print(\"Model saved as 'genre_resnet18_model_full_epochs.pth'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Convert To Spectogram**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIUN5tI9aLun",
        "outputId": "c04572fa-3bce-4ab7-fccd-28401de2cf38"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Input and output folders\n",
        "input_root = '/content/drive/MyDrive/dataset dl/data primer'\n",
        "output_root = '/content/drive/MyDrive/dataset dl/waveform secondary data'\n",
        "\n",
        "# Loop through genre folders\n",
        "for genre in os.listdir(input_root):\n",
        "    genre_path = os.path.join(input_root, genre)\n",
        "    if not os.path.isdir(genre_path):\n",
        "        continue\n",
        "\n",
        "    output_genre_path = os.path.join(output_root, genre)\n",
        "    os.makedirs(output_genre_path, exist_ok=True)\n",
        "\n",
        "    # Loop through each audio file\n",
        "    for filename in os.listdir(genre_path):\n",
        "        if not filename.lower().endswith(('.wav', '.mp3', '.au')):\n",
        "            continue\n",
        "\n",
        "        filepath = os.path.join(genre_path, filename)\n",
        "        try:\n",
        "            y, sr = librosa.load(filepath, sr=None, mono=True)\n",
        "\n",
        "            S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "            S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "            plt.figure(figsize=(2.56, 2.56), dpi=50)  # 128x128 pixels\n",
        "            librosa.display.specshow(S_DB, sr=sr, cmap='magma')\n",
        "            plt.axis('off')\n",
        "\n",
        "            output_file = os.path.join(output_genre_path, filename.rsplit('.', 1)[0] + '.png')\n",
        "            plt.savefig(output_file, bbox_inches='tight', pad_inches=0)\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"[✔] Saved: {output_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[✘] Failed to process {filepath}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[✓] File: rock.00073.png | Predicted: rock | Actual: rock\n",
            "[✓] File: rock.00091.png | Predicted: rock | Actual: rock\n",
            "[✓] File: Oasis - Don’t Look Back In Anger.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00069.png | Predicted: pop | Actual: hiphop\n",
            "[✓] File: rock.00015.png | Predicted: hiphop | Actual: rock\n",
            "[✓] File: rock.00034.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00009.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: pop.00009.png | Predicted: pop | Actual: pop\n",
            "[✓] File: hiphop.00042.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: rock.00059.png | Predicted: rock | Actual: rock\n",
            "[✓] File: classical.00081.png | Predicted: classical | Actual: classical\n",
            "[✓] File: rock.00021.png | Predicted: rock | Actual: rock\n",
            "[✓] File: classical.00092.png | Predicted: classical | Actual: classical\n",
            "[✓] File: rock.00084.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00085.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00009.png | Predicted: classical | Actual: classical\n",
            "[✓] File: hiphop.00050.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: pop.00033.png | Predicted: pop | Actual: pop\n",
            "[✓] File: pop.00028.png | Predicted: pop | Actual: pop\n",
            "[✓] File: classical.00030.png | Predicted: classical | Actual: classical\n",
            "[✓] File: pop.00048.png | Predicted: pop | Actual: pop\n",
            "[✓] File: hiphop.00032.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00040.png | Predicted: classical | Actual: classical\n",
            "[✓] File: pop.00017.png | Predicted: pop | Actual: pop\n",
            "[✓] File: rock.00099.png | Predicted: classical | Actual: rock\n",
            "[✓] File: pop.00069.png | Predicted: hiphop | Actual: pop\n",
            "[✓] File: Menari-Maliq-_-D’Essentials-_-Lirik-Lagu.png | Predicted: rock | Actual: pop\n",
            "[✓] File: classical.00084.png | Predicted: classical | Actual: classical\n",
            "[✓] File: Georges Bizet -  Les Toreadors  from Carmen Suite No. 1.png | Predicted: rock | Actual: classical\n",
            "[✓] File: hiphop.00049.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: C.H.R.I.S.Y.E..png | Predicted: classical | Actual: pop\n",
            "[✓] File: rock.00075.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00078.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: hiphop.00058.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00015.png | Predicted: classical | Actual: classical\n",
            "[✓] File: pop.00073.png | Predicted: pop | Actual: pop\n",
            "[✓] File: classical.00000.png | Predicted: classical | Actual: classical\n",
            "[✓] File: hiphop.00079.png | Predicted: pop | Actual: hiphop\n",
            "[✓] File: rock.00081.png | Predicted: rock | Actual: rock\n",
            "[✓] File: N.W.A. - Fuk Da Police.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: pop.00004.png | Predicted: pop | Actual: pop\n",
            "[✓] File: rock.00028.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00095.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: hiphop.00022.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: pop.00044.png | Predicted: pop | Actual: pop\n",
            "[✓] File: pop.00022.png | Predicted: pop | Actual: pop\n",
            "[✓] File: Johann Strauss II - Voices of Spring Waltz.png | Predicted: classical | Actual: classical\n",
            "[✓] File: rock.00036.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00066.png | Predicted: rock | Actual: hiphop\n",
            "[✓] File: rock.00093.png | Predicted: rock | Actual: rock\n",
            "[✓] File: classical.00095.png | Predicted: classical | Actual: classical\n",
            "[✓] File: pop.00031.png | Predicted: pop | Actual: pop\n",
            "[✓] File: pop.00018.png | Predicted: pop | Actual: pop\n",
            "[✓] File: rock.00044.png | Predicted: classical | Actual: rock\n",
            "[✓] File: hiphop.00026.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: pop.00030.png | Predicted: pop | Actual: pop\n",
            "[✓] File: pop.00039.png | Predicted: pop | Actual: pop\n",
            "[✓] File: pop.00074.png | Predicted: pop | Actual: pop\n",
            "[✓] File: hiphop.00008.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: NIKI-Every-Summertime-_Lyrics_-Every-year-we-get-older.png | Predicted: rock | Actual: pop\n",
            "[✓] File: pop.00067.png | Predicted: pop | Actual: pop\n",
            "[✓] File: classical.00038.png | Predicted: classical | Actual: classical\n",
            "[✓] File: pop.00041.png | Predicted: rock | Actual: pop\n",
            "[✓] File: hiphop.00031.png | Predicted: pop | Actual: hiphop\n",
            "[✓] File: classical.00098.png | Predicted: classical | Actual: classical\n",
            "[✓] File: classical.00086.png | Predicted: classical | Actual: classical\n",
            "[✓] File: rock.00043.png | Predicted: pop | Actual: rock\n",
            "[✓] File: hiphop.00025.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: hiphop.00068.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: pop.00007.png | Predicted: pop | Actual: pop\n",
            "[✓] File: hiphop.00055.png | Predicted: rock | Actual: hiphop\n",
            "[✓] File: pop.00084.png | Predicted: rock | Actual: pop\n",
            "[✓] File: hiphop.00091.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: rock.00053.png | Predicted: pop | Actual: rock\n",
            "[✓] File: classical.00067.png | Predicted: classical | Actual: classical\n",
            "[✓] File: hiphop.00071.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: hiphop.00006.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: pop.00050.png | Predicted: pop | Actual: pop\n",
            "[✓] File: classical.00055.png | Predicted: classical | Actual: classical\n",
            "[✓] File: Bon Jovi - It s My Life [Lyrics].png | Predicted: rock | Actual: rock\n",
            "[✓] File: rock.00018.png | Predicted: hiphop | Actual: rock\n",
            "[✓] File: pop.00020.png | Predicted: pop | Actual: pop\n",
            "[✓] File: pop.00081.png | Predicted: rock | Actual: pop\n",
            "[✓] File: classical.00005.png | Predicted: classical | Actual: classical\n",
            "[✓] File: pop.00058.png | Predicted: pop | Actual: pop\n",
            "[✓] File: hiphop.00056.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00076.png | Predicted: classical | Actual: classical\n",
            "[✓] File: hiphop.00053.png | Predicted: pop | Actual: hiphop\n",
            "[✓] File: Big Sean - I Don t Fuck With You ft. E-40 (Official Lyric Video).png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: pop.00053.png | Predicted: pop | Actual: pop\n",
            "[✓] File: pop.00071.png | Predicted: pop | Actual: pop\n",
            "[✓] File: hiphop.00074.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00053.png | Predicted: classical | Actual: classical\n",
            "[✓] File: hiphop.00096.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00008.png | Predicted: classical | Actual: classical\n",
            "[✓] File: classical.00032.png | Predicted: classical | Actual: classical\n",
            "[✓] File: hiphop.00047.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00014.png | Predicted: classical | Actual: classical\n",
            "[✓] File: rock.00068.png | Predicted: rock | Actual: rock\n",
            "[✓] File: rock.00060.png | Predicted: pop | Actual: rock\n",
            "[✓] File: pop.00035.png | Predicted: pop | Actual: pop\n",
            "[✓] File: rock.00082.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00065.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: pop.00010.png | Predicted: rock | Actual: pop\n",
            "[✓] File: hiphop.00038.png | Predicted: rock | Actual: hiphop\n",
            "[✓] File: classical.00021.png | Predicted: classical | Actual: classical\n",
            "[✓] File: classical.00019.png | Predicted: classical | Actual: classical\n",
            "[✓] File: rock.00062.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00010.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00049.png | Predicted: classical | Actual: classical\n",
            "[✓] File: classical.00093.png | Predicted: classical | Actual: classical\n",
            "[✓] File: rock.00076.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00064.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00088.png | Predicted: classical | Actual: classical\n",
            "[✓] File: classical.00063.png | Predicted: classical | Actual: classical\n",
            "[✓] File: hiphop.00036.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00037.png | Predicted: classical | Actual: classical\n",
            "[✓] File: hiphop.00011.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: Kashmir (Remaster).png | Predicted: rock | Actual: rock\n",
            "[✓] File: Aitch x AJ Tracey - Rain Feat. Tay Keith (Official Video).png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: hiphop.00061.png | Predicted: rock | Actual: hiphop\n",
            "[✓] File: pop.00025.png | Predicted: pop | Actual: pop\n",
            "[✓] File: hiphop.00073.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: hiphop.00015.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: rock.00045.png | Predicted: rock | Actual: rock\n",
            "[✓] File: rock.00012.png | Predicted: hiphop | Actual: rock\n",
            "[✓] File: pop.00008.png | Predicted: pop | Actual: pop\n",
            "[✓] File: classical.00052.png | Predicted: rock | Actual: classical\n",
            "[✓] File: rock.00024.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00044.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: rock.00047.png | Predicted: hiphop | Actual: rock\n",
            "[✓] File: hiphop.00093.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: classical.00028.png | Predicted: classical | Actual: classical\n",
            "[✓] File: pop.00092.png | Predicted: pop | Actual: pop\n",
            "[✓] File: rock.00097.png | Predicted: rock | Actual: rock\n",
            "[✓] File: pop.00005.png | Predicted: rock | Actual: pop\n",
            "[✓] File: classical.00090.png | Predicted: classical | Actual: classical\n",
            "[✓] File: classical.00026.png | Predicted: classical | Actual: classical\n",
            "[✓] File: rock.00035.png | Predicted: rock | Actual: rock\n",
            "[✓] File: pop.00057.png | Predicted: pop | Actual: pop\n",
            "[✓] File: Money Trees.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: Hotel California - Eagle (Lyric).png | Predicted: rock | Actual: rock\n",
            "[✓] File: rock.00003.png | Predicted: hiphop | Actual: rock\n",
            "[✓] File: pop.00036.png | Predicted: pop | Actual: pop\n",
            "[✓] File: Guns N Roses - Knockin On Heaven s Door (Visualizer).png | Predicted: hiphop | Actual: rock\n",
            "[✓] File: rock.00029.png | Predicted: rock | Actual: rock\n",
            "[✓] File: pop.00054.png | Predicted: pop | Actual: pop\n",
            "[✓] File: classical.00022.png | Predicted: classical | Actual: classical\n",
            "[✓] File: rock.00023.png | Predicted: rock | Actual: rock\n",
            "[✓] File: pop.00098.png | Predicted: pop | Actual: pop\n",
            "[✓] File: rock.00079.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00012.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: pop.00016.png | Predicted: pop | Actual: pop\n",
            "[✓] File: rock.00046.png | Predicted: hiphop | Actual: rock\n",
            "[✓] File: pop.00086.png | Predicted: pop | Actual: pop\n",
            "[✓] File: rock.00057.png | Predicted: rock | Actual: rock\n",
            "[✓] File: hiphop.00084.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: rock.00061.png | Predicted: hiphop | Actual: rock\n",
            "[✓] File: pop.00064.png | Predicted: hiphop | Actual: pop\n",
            "[✓] File: classical.00071.png | Predicted: classical | Actual: classical\n",
            "[✓] File: classical.00096.png | Predicted: classical | Actual: classical\n",
            "[✓] File: hiphop.00021.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: hiphop.00045.png | Predicted: hiphop | Actual: hiphop\n",
            "[✓] File: rock.00038.png | Predicted: hiphop | Actual: rock\n",
            "[✓] File: hiphop.00002.png | Predicted: rock | Actual: hiphop\n",
            "[✓] File: rock.00007.png | Predicted: rock | Actual: rock\n",
            "[✓] File: pop.00080.png | Predicted: pop | Actual: pop\n",
            "[✓] File: pop.00066.png | Predicted: pop | Actual: pop\n",
            "[✓] File: classical.00002.png | Predicted: classical | Actual: classical\n",
            "[✓] File: hiphop.00028.png | Predicted: rock | Actual: hiphop\n",
            "[✓] File: classical.00011.png | Predicted: classical | Actual: classical\n",
            "[✓] File: pop.00085.png | Predicted: pop | Actual: pop\n",
            "\n",
            "Test Accuracy: 79.07%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "# Assuming 'model', 'test_loader', 'device' are already defined and 'full_dataset' from the splitting example.\n",
        "\n",
        "# Evaluation mode\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# --- FIX START ---\n",
        "# Get class names from the original full_dataset\n",
        "class_names = full_dataset.classes\n",
        "\n",
        "# Get all file paths from the original full_dataset\n",
        "# test_dataset is a Subset, so its 'samples' are not directly accessible in the same way.\n",
        "# We need to map the indices of the test_dataset (Subset) back to the original full_dataset's samples.\n",
        "original_file_paths = [path for path, _ in full_dataset.samples]\n",
        "\n",
        "# Get the indices of the test_dataset (Subset)\n",
        "test_indices = test_dataset.indices\n",
        "\n",
        "# Create a list of file paths specifically for the test set\n",
        "test_file_paths = [original_file_paths[i] for i in test_indices]\n",
        "\n",
        "file_idx = 0 # This index will now correctly iterate through test_file_paths\n",
        "# --- FIX END ---\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            # Use test_file_paths for getting the filename\n",
        "            filename = os.path.basename(test_file_paths[file_idx])\n",
        "            true_class = class_names[labels[i].item()]\n",
        "            pred_class = class_names[predicted[i].item()]\n",
        "            print(f\"[✓] File: {filename} | Predicted: {pred_class} | Actual: {true_class}\")\n",
        "            file_idx += 1\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'\\nTest Accuracy: {accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "envir",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
